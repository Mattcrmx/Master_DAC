dim_emb: 50
dim_h: 256
n_layers: 0
lr: 0.001
batch_size: 64
v_net: False
k_net: False
model_type: 'complex_attention'
trainer:
  # args for PyTorch Lightning Trainer
  gpus: 0
  max_epochs: 1000
  log_every_n_steps: 10
