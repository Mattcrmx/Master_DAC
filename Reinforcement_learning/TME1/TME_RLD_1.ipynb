{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTR = \"./CTR.txt\"\n",
    "\n",
    "with open(CTR, \"r\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "content = [x.split(':') for x in content]\n",
    "\n",
    "articles = [x[1].split(';') for x in content]\n",
    "clicks = [x[2].split(';') for x in content]\n",
    "\n",
    "articles = [[float(x_i) for x_i in x] for x in articles]\n",
    "clicks = [[float(x_i) for x_i in x] for x in clicks]\n",
    "\n",
    "clicks = np.array(clicks)\n",
    "articles = np.array(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy:\n",
    "    def __init__(self, strat, articles, clicks, delta=0.05, alpha=None):\n",
    "        self.strat = strat\n",
    "        self.articles = articles\n",
    "        self.clicks = clicks\n",
    "        self.score = 0\n",
    "        self.static_best = np.argmax(np.sum(clicks, axis=0))\n",
    "        \n",
    "        # for UCB\n",
    "        self.reward_history = np.zeros(10)\n",
    "        self.choice_history = np.zeros(10)\n",
    "        \n",
    "        # LinUCB\n",
    "        self.A_list = [np.eye(5) for _ in range(10)]\n",
    "        self.b_list = [np.zeros(5) for _ in range(10)]\n",
    "        if alpha is None:\n",
    "            self.alpha = 1 + np.sqrt(np.log(2 / delta) / 2)\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "            \n",
    "        # Thompson Sampling for Contextual Bandits (Agrawal & Goyal, 2013):\n",
    "        # not applicable here, as we have a global context vector x_t all machines (announcers),\n",
    "        # not individual context vectors x_it\n",
    "        \n",
    "        \n",
    "    def select(self, t):\n",
    "        if self.strat == 'Random':\n",
    "            return random.randint(0,9)\n",
    "        \n",
    "        if self.strat == 'StaticBest':\n",
    "            return self.static_best\n",
    "        \n",
    "        if self.strat == 'Optimal':\n",
    "            return np.argmax(clicks[t])\n",
    "        \n",
    "        if self.strat == 'UCB':\n",
    "            ub_estimate = [1/self.choice_history[i]*self.reward_history[i] \n",
    "                             + np.sqrt(2*np.log(t) / self.choice_history[i]) for i in range(10)]\n",
    "            \n",
    "            choice = np.argmax(ub_estimate)\n",
    "            \n",
    "            self.reward_history[choice] += self.clicks[t, choice]\n",
    "            self.choice_history[choice] += 1\n",
    "            return choice\n",
    "        \n",
    "        if 'LinUCB' in self.strat:\n",
    "            x_t = self.articles[t]\n",
    "            piche = np.zeros(10)\n",
    "            \n",
    "            for i in range(10):\n",
    "                A = self.A_list[i]\n",
    "                b = self.b_list[i]\n",
    "                A_inv = np.linalg.inv(A)\n",
    "                \n",
    "                theta = A_inv.dot(b)\n",
    "                piche[i] = theta.dot(x_t) + self.alpha * np.sqrt(x_t.dot(A_inv).dot(x_t))\n",
    "                \n",
    "            choice = np.argmax(piche)\n",
    "            r_t = self.clicks[t, choice]\n",
    "\n",
    "            self.A_list[choice] += x_t.dot(x_t)\n",
    "            self.b_list[choice] += r_t * x_t\n",
    "            return choice\n",
    "        \n",
    "        if 'Thompson' in self.strat:\n",
    "            x_t = self.articles[t]\n",
    "            \n",
    "            # Sample a theta\n",
    "            A_inv = np.linalg.inv(self.A)\n",
    "            theta = np.random.multivariate_normal(self.b.dot(A_inv), A_inv)\n",
    "            \n",
    "            # Find the arm that maximizes our reward expectancy\n",
    "            reward_expectancies = [theta.dot(x_t) for _ in]\n",
    "\n",
    "                \n",
    "        \n",
    "    def reward(self, t):\n",
    "        return clicks[t][self.select(t)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_names = ['Random', 'StaticBest', 'Optimal', 'UCB', 'LinUCB-0.15']\n",
    "strategies = {}\n",
    "for strat in strat_names:\n",
    "    if 'LinUCB' in strat:\n",
    "        alpha = float(strat.split('-')[1])\n",
    "        strategies[strat] = Strategy(strat, articles, clicks, alpha=alpha)\n",
    "    else:\n",
    "        strategies[strat] = Strategy(strat, articles, clicks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines & UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = []\n",
    "total_regret = []\n",
    "\n",
    "for t in range(len(clicks)):\n",
    "    rewards = []\n",
    "    regrets = []\n",
    "    r_t_staticbest = clicks[t, strategies['StaticBest'].select(t)]\n",
    "    \n",
    "    for strat in strat_names:\n",
    "        i = strategies[strat].select(t)\n",
    "        r_ti = clicks[t, i]\n",
    "        rewards.append(r_ti)\n",
    "        regrets.append(r_t_staticbest - r_ti)\n",
    "        \n",
    "    total_reward.append(rewards)\n",
    "    total_regret.append(regrets)\n",
    "    \n",
    "total_regret = np.cumsum(np.array(total_regret), axis=0)\n",
    "total_reward = np.cumsum(np.array(total_reward), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for k, s in enumerate(strat_names):\n",
    "    plt.plot(total_reward[:, k], label=s)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for k, s in enumerate(strat_names):\n",
    "    plt.plot(total_regret[:, k], label=s)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Bandits : Lin-UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_regret(alphas):\n",
    "    lin_ucb_strategies = [Strategy('LinUCB', articles, clicks, alpha=a) for a in alphas]\n",
    "\n",
    "    total_reward = []\n",
    "    total_regret = []\n",
    "\n",
    "    for t in range(len(clicks)):\n",
    "        if t%1000 == 0:\n",
    "            print(f'{t}/{len(clicks)}')\n",
    "        rewards = []\n",
    "        regrets = []\n",
    "        r_t_staticbest = clicks[t, strategies['StaticBest'].select(t)]\n",
    "\n",
    "        for strat in lin_ucb_strategies:\n",
    "            i = strat.select(t)\n",
    "            r_ti = clicks[t, i]\n",
    "            rewards.append(r_ti)\n",
    "            regrets.append(r_t_staticbest - r_ti)\n",
    "\n",
    "        total_reward.append(rewards)\n",
    "        total_regret.append(regrets)\n",
    "\n",
    "    total_regret = np.cumsum(np.array(total_regret), axis=0)\n",
    "    total_reward = np.cumsum(np.array(total_reward), axis=0)\n",
    "\n",
    "    return total_regret[-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounds that make sense for alpha in a statistical setting (with a 1-$\\delta$ confidence level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = np.logspace(-2.5, 0, num=20)\n",
    "alphas = np.array([1 + np.sqrt(np.log(2 / delta) / 2) for delta in deltas])\n",
    "plt.plot(deltas, alphas)\n",
    "plt.xlabel('delta')\n",
    "plt.ylabel('alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_regret = get_final_regret(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(221)\n",
    "plt.plot(alphas, final_regret)\n",
    "plt.ylabel('final regret')\n",
    "plt.xlabel('alpha')\n",
    "plt.subplot(222)\n",
    "plt.plot(deltas, final_regret)\n",
    "plt.ylabel('final regret')\n",
    "plt.xlabel('delta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in reality we can tune alpha directly to go further in both directions:\n",
    "- bigger delta = smaller confidence intervals, i.e. more confidence in our estimates => smaller alpha & less exploration\n",
    "- smaller delta/bigger alpha => more exploration\n",
    "\n",
    "Let's explore the hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-2, 2, num=100)\n",
    "final_regret = get_final_regret(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, final_regret)\n",
    "plt.ylabel('final regret')\n",
    "plt.xlabel('alpha')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-1, -0.5, num=200)\n",
    "final_regret = get_final_regret(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, final_regret)\n",
    "plt.ylabel('final regret')\n",
    "plt.xlabel('alpha')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.argmin(final_regret)\n",
    "print(f'Best alpha: {alphas[i]}, final regret: {final_regret[i]} (run {i})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
